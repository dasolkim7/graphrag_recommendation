import torch
import torch.nn.functional as F
from torch_geometric.nn import GATConv
import faiss
import numpy as np
from collections import defaultdict
import os

def main():
    print("ğŸ”„ Loading data safely...")
    try:
        saved = torch.load("full_graph.pt", weights_only=False)
        data = saved['data']
        node_meta = saved['node_meta']
        NODE_TYPE_MAP = saved['NODE_TYPE_MAP']
    except FileNotFoundError:
        print("âŒ 'full_graph.pt' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ê·¸ë˜í”„ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.")
        return

    # SceneWindow ì¸ë±ìŠ¤ ì¶”ì¶œ
    sw_mask = (data.node_type == NODE_TYPE_MAP['SceneWindow'])
    sw_indices = sw_mask.nonzero(as_tuple=True)[0]

    # --- ì†ì‹¤ í•¨ìˆ˜ & Positive Pair ìƒì„± ë¡œì§ ---
    def find_positive_pairs(node_meta, sw_indices):
        """ê°™ì€ ì˜í™”ì˜ ì¸ì ‘ ìœˆë„ìš°ë¥¼ positive pairë¡œ ì •ì˜."""
        movie_windows = defaultdict(list)
        
        # SceneWindow ê¸€ë¡œë²Œ ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì™€ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
        for idx_tensor in sw_indices:
            idx = idx_tensor.item()
            meta = node_meta[idx]
            if meta.get('window') is not None:
                movie_windows[meta['movie']].append((idx, meta['window']))
        
        pairs = []
        # ì˜í™”ë³„ë¡œ ìœˆë„ìš° ë²ˆí˜¸ ìˆœ ì •ë ¬ í›„ ì¸ì ‘ ìŒ ìƒì„±
        for movie, windows in movie_windows.items():
            windows.sort(key=lambda x: x[1])  # window number ì˜¤ë¦„ì°¨ìˆœ
            for i in range(len(windows) - 1):
                # (global_idx1, global_idx2)
                pairs.append((windows[i][0], windows[i+1][0]))
        
        if not pairs:
            return torch.empty((2, 0), dtype=torch.long)
            
        return torch.tensor(pairs, dtype=torch.long).t()  # [2, num_pairs]

    def contrastive_loss(embeddings, positive_pairs, temperature=0.5):
        """
        embeddings: ì „ì²´ ë…¸ë“œ ì„ë² ë”© [N, dim]
        positive_pairs: [2, num_pairs] - (idx_a, idx_b)
        """
        idx_a, idx_b = positive_pairs
        
        anchor = embeddings[idx_a]
        positive = embeddings[idx_b]
        
        # Cosine Similarity
        pos_sim = F.cosine_similarity(anchor, positive, dim=-1) / temperature
        
        # Negative Sampling (Shuffle)
        # ë°°ì¹˜ ë‚´ì—ì„œ ëœë¤í•˜ê²Œ ì„ì€ ê²ƒì„ negativeë¡œ ê°„ì£¼
        batch_size = anchor.size(0)
        perm = torch.randperm(batch_size)
        negative = anchor[perm]

        neg_sim = F.cosine_similarity(anchor, negative, dim=-1) / temperature
        
        # InfoNCE Loss
        logits = torch.stack([pos_sim, neg_sim], dim=1) # [batch, 2]
        labels = torch.zeros(batch_size, dtype=torch.long, device=embeddings.device) # 0ë²ˆì§¸ê°€ positive
        
        return F.cross_entropy(logits, labels)

    # Positive Pairs ë¯¸ë¦¬ ê³„ì‚°
    print("ğŸ”— Generating positive pairs (finding adjacent windows in same movies)...")
    pos_edge_index = find_positive_pairs(node_meta, sw_indices)
    print(f"âœ… Positive Pairs created: {pos_edge_index.size(1)} pairs")

    if pos_edge_index.size(1) == 0:
        print("âš ï¸ Warning: No positive pairs found. Training might be unstable.")

    # 2. GNN ëª¨ë¸ ì •ì˜ (GAT)
    class NarrativeGNN(torch.nn.Module):
        def __init__(self, in_dim=43, hidden_dim=128, out_dim=256):
            super().__init__()
            self.conv1 = GATConv(in_dim, hidden_dim, heads=4, dropout=0.2)
            self.conv2 = GATConv(hidden_dim * 4, out_dim, heads=1, dropout=0.2)
        
        def forward(self, x, edge_index):
            x = F.elu(self.conv1(x, edge_index))
            x = self.conv2(x, edge_index)
            return x

    model = NarrativeGNN()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    # 3. í•™ìŠµ ë£¨í”„
    print("ğŸš€ GNN Training Start (Contrastive Learning)...")
    model.train()
    for epoch in range(100): 
        optimizer.zero_grad()
        
        # ì „ì²´ ì„ë² ë”© ìƒì„±
        out = model(data.x, data.edge_index)
        
        # Contrastive Loss (Positive Pair + Negative Sampling)
        if pos_edge_index.size(1) > 0:
            loss = contrastive_loss(out, pos_edge_index)
        else:
             # Fallback if no pairs (shouldn't happen in this dataset)
             loss = torch.tensor(0.0, requires_grad=True)
        
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/100 | Loss: {loss.item():.4f}")

    # 1. ì„ë² ë”© ì¶”ì¶œ ë° L2 ì •ê·œí™”
    model.eval()
    with torch.no_grad():
        final_out = model(data.x, data.edge_index)
        sw_embeddings = final_out[sw_indices].cpu().numpy()
    faiss.normalize_L2(sw_embeddings)

    # 2. ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥
    d = sw_embeddings.shape[1]
    new_index = faiss.IndexFlatIP(d)  # Cosine Similarityìš©
    new_index.add(sw_embeddings)

    # íŒŒì¼ ì €ì¥
    faiss.write_index(new_index, "narrative_index.faiss")
    np.save("sw_embeddings.npy", sw_embeddings)

    print(f"âœ… Files saved: {os.path.abspath('narrative_index.faiss')}")
    print(f"âœ… Vector count: {new_index.ntotal}")

if __name__ == "__main__":
    main()
